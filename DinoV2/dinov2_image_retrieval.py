# -*- coding: utf-8 -*-
"""dinov2-image-retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb

![Roboflow Notebooks banner](https://camo.githubusercontent.com/aec53c2b5fb6ed43d202a0ab622b58ba68a89d654fbe3abab0c0cc8bd1ff424e/68747470733a2f2f696b2e696d6167656b69742e696f2f726f626f666c6f772f6e6f7465626f6f6b732f74656d706c6174652f62616e6e657274657374322d322e706e673f696b2d73646b2d76657273696f6e3d6a6176617363726970742d312e342e33267570646174656441743d31363732393332373130313934)

# Image Retrieval with DINOv2

DINOv2, released by Meta Research in April 2023, implements a self-supervised method of training computer vision models.

DINOv2 was trained using 140 million images without labels. The embeddings generated by DINOv2 can be used for classification, image retrieval, segmentation, and depth estimation. With that said, Meta Research did not release heads for segmentation and depth estimation.

In this guide, we're going to talk through how to build an image-to-image retrieval system with DINOv2. We will:

1. Load a folder of images
2. Compute embeddings for each image
3. Save all the embeddings in a `faiss` index
4. Build the logic to search the index for an image

By the end of this notebook, we'll have a system that we can feed an image to return related images.

Without further ado, let's begin!

## Load Images

In this demo, we're going to build an image search engine for the [COCO 128 dataset](https://universe.roboflow.com/team-roboflow/coco-128) available on Roboflow Universe. First, let's download the dataset from Roboflow and make a list of all the files in the `train` folder on which we'll build our search engine.

To download this dataset, you will need a [free Roboflow account](https://app.roboflow.com).

_Note: You can provide any folder of images you want!_
"""

"""
Before:
pip install roboflow faiss-cpu supervision -q"
pip install roboflow
roboflow login

import roboflow
import os


roboflow.login()

rf = roboflow.Roboflow()

project = rf.workspace("team-roboflow").project("coco-128")
dataset = project.version(2).download("coco")
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
cwd = os.getcwd()
ROOT_DIR = os.path.join(cwd, "C:/Users/franc/OneDrive/Desktop/Secondo semestre 24-25/Machine Learning/project/training/American_chameleon")

files = os.listdir(ROOT_DIR)
files = [os.path.join(ROOT_DIR, f) for f in files if f.lower().endswith(".jpg")]

"""## Import Dependencies and Load Model

Next, let's import the remaining dependencie required for our project and load the DINOv2 model. We'll work with the smallest model, `dinov2_vits14` in this guide. You can swap this model out for any checkpoint from the DINOv2 GitHub repository.

We'll also set a `ROOT_DIR`. This refers to the folder in which the images we want to search are stored.
"""

import faiss
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
import cv2
import json
from tqdm.notebook import tqdm
from matplotlib import pyplot as plt
import supervision as sv

dinov2_vits14 = torch.hub.load("facebookresearch/dinov2", "dinov2_vits14")

device = torch.device('cuda' if torch.cuda.is_available() else "cpu")

dinov2_vits14.to(device)

"""## Prepare Images and Index

To build an image-to-image search engine, we need to store:

1. An embedding representation of each image in our dataset, for use in querying for related images, and;
2. An index that maps the index of an embedding in the `faiss` data store to file names (faiss associates values with indices, so we need a way to map these indices back to file names).

Before we can build the index, we need to transform an image into a format that the DINOv2 ViT model can understand. We do this in the `transform_image` and `load_image` methods below.

Then, we define a function that iterates through each of the files in a specified folder and computes embeddings for each file. These embeddings are stored in a `faiss` vector index that we will query to find images.
"""

transform_image = T.Compose([T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])])

def load_image(img: str) -> torch.Tensor:
    """
    Load an image and return a tensor that can be used as an input to DINOv2.
    """
    img = Image.open(img)

    transformed_img = transform_image(img)[:3].unsqueeze(0)

    return transformed_img

def create_index(files: list) -> faiss.IndexFlatL2:
    """
    Create an index that contains all of the images in the specified list of files.
    """
    index = faiss.IndexFlatL2(384)

    all_embeddings = {}

    with torch.no_grad():
      for i, file in enumerate(files):
        embeddings = dinov2_vits14(load_image(file).to(device))

        embedding = embeddings[0].cpu().numpy()

        all_embeddings[file] = np.array(embedding).reshape(1, -1).tolist()

        index.add(np.array(embedding).reshape(1, -1))

    with open("all_embeddings.json", "w") as f:
        f.write(json.dumps(all_embeddings))

    faiss.write_index(index, "data.bin")

    return index, all_embeddings

data_index, all_embeddings = create_index(files)

"""## Prepare to Search the Index

Below, we define a function that retrieves `k` images with embeddings most similar to the embedding of an input image from our vector index. In other words, this function will search for related images in our index and return their positions in the index.
"""

def search_index(index: faiss.IndexFlatL2, embeddings: list, k: int = 3) -> list:
    """
    Search the index for the images that are most similar to the provided image.
    """
    D, I = index.search(np.array(embeddings[0].reshape(1, -1)), k)

    return I[0]

"""## Search the Index

The code below takes an input image (`search_file`), calculates an embedding for the image, and uses that embedding to find related images.

We display the `k` top results (default 3) in the notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
search_file = "C:/Users/franc/OneDrive/Desktop/Secondo semestre 24-25/Machine Learning/project/test/query/n01855672_10973.jpg"

img = cv2.resize(cv2.imread(search_file), (416, 416))

print("Input image:")

# %matplotlib inline
sv.plot_image(image=img, size=(16, 16))

print("*" * 20)

with torch.no_grad():
    embedding = dinov2_vits14(load_image(search_file).to(device))

    indices = search_index(data_index, np.array(embedding[0].cpu()).reshape(1, -1))

    for i, index in enumerate(indices):
        print()
        print(f"Image {i}: {files[index]}")
        img = cv2.resize(cv2.imread(files[index]), (416, 416))
#         %matplotlib inline
        sv.plot_image(image=img, size=(16, 16))

"""## üèÜ Congratulations

### Learning Resources

Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:

- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.
- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.
- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.
- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.

### Convert data formats

Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.

### Connect computer vision to your project logic

[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections.
"""